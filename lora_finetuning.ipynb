{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning with MLflow\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model using LoRA (Low-Rank Adaptation) with comprehensive MLflow tracking.\n",
    "\n",
    "## What you'll learn:\n",
    "- LoRA fine-tuning of small language models\n",
    "- MLflow experiment tracking and model registry\n",
    "- Model evaluation and comparison\n",
    "- Creating instruction-following datasets\n",
    "\n",
    "## Requirements:\n",
    "- Python 3.8+\n",
    "- GPU recommended (but works on CPU/MPS)\n",
    "- ~4GB RAM for small models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# ML and Deep Learning\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable tokenizers warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üìä MLflow version: {mlflow.__version__}\")\n",
    "print(f\"üìà tqdm available for progress tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "Let's define our training configuration and set up MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"microsoft/DialoGPT-small\",  # Smaller model for demo\n",
    "    \"max_length\": 256,\n",
    "    \n",
    "    # Training settings\n",
    "    \"batch_size\": 2,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"num_epochs\": 1,  # Reduced for demo\n",
    "    \"warmup_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"lora_r\": 8,  # Rank\n",
    "    \"lora_alpha\": 16,  # Alpha parameter\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"c_attn\", \"c_proj\"],  # DialoGPT specific modules\n",
    "    \n",
    "    # Output settings\n",
    "    \"output_dir\": \"./lora_model\",\n",
    "    \"experiment_name\": \"lora_finetuning_demo\"\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"üöÄ Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üçé Using Apple Silicon MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "print(f\"üìã Configuration loaded: {config['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLflow Setup\n",
    "\n",
    "Initialize MLflow for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow for local tracking (ROBUST VERSION)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Clear ALL MLflow environment variables that might cause issues\n",
    "mlflow_env_vars = [\n",
    "    'MLFLOW_TRACKING_URI', \n",
    "    'MLFLOW_TRACKING_URL', \n",
    "    'MLFLOW_SERVER_HOST', \n",
    "    'MLFLOW_SERVER_PORT',\n",
    "    'MLFLOW_REGISTRY_URI'\n",
    "]\n",
    "\n",
    "for var in mlflow_env_vars:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "        print(f\"üßπ Cleared environment variable: {var}\")\n",
    "\n",
    "# Step 2: Force local MLflow tracking\n",
    "mlruns_dir = Path(\"./mlruns\").resolve()\n",
    "mlruns_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Step 3: Set tracking URI to local file system BEFORE any other MLflow operations\n",
    "tracking_uri = f\"file://{mlruns_dir}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"üè† Using local MLflow tracking: {tracking_uri}\")\n",
    "print(f\"üìç MLruns directory: {mlruns_dir}\")\n",
    "\n",
    "# Step 4: Verify the tracking URI is set correctly\n",
    "current_uri = mlflow.get_tracking_uri()\n",
    "print(f\"üîç Current MLflow URI: {current_uri}\")\n",
    "\n",
    "# Step 5: Set up MLflow experiment\n",
    "try:\n",
    "    mlflow.set_experiment(config[\"experiment_name\"])\n",
    "    print(f\"‚úÖ MLflow experiment created: {config['experiment_name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating experiment: {e}\")\n",
    "    print(\"üîß Trying to create experiment with explicit local tracking...\")\n",
    "    \n",
    "    # Force set tracking URI again and retry\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(config[\"experiment_name\"])\n",
    "    print(f\"‚úÖ MLflow experiment created (retry): {config['experiment_name']}\")\n",
    "\n",
    "print(f\"üîó MLflow UI: http://localhost:5000 (run 'mlflow ui' in terminal)\")\n",
    "print(\"üí° Note: You don't need to run MLflow UI for the notebook to work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Training Dataset\n",
    "\n",
    "Let's create a simple instruction-following dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset():\n",
    "    \"\"\"Create a simple instruction-following dataset.\"\"\"\n",
    "    \n",
    "    training_data = [\n",
    "        {\n",
    "            \"instruction\": \"What is Unsloth and why use it for fine-tuning LLMs?\",\n",
    "            \"output\": \"Unsloth is a toolkit focused on faster, more memory-efficient fine-tuning of LLMs. It wraps model loading and LoRA/QLoRA setup with optimized kernels and paging optimizers, often yielding lower VRAM usage and higher throughput while staying compatible with Hugging Face/TRL.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I install Unsloth for a typical CUDA setup?\",\n",
    "            \"output\": \"Create a clean environment, install PyTorch with CUDA, then `pip install unsloth`. If you need bleeding-edge features, install from GitHub. Ensure your CUDA and driver versions match the installed PyTorch build.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Can I use Unsloth on a MacBook without NVIDIA GPUs?\",\n",
    "            \"output\": \"Yes for small models via CPU or Apple MPS, but the biggest speedups come from NVIDIA CUDA GPUs. For larger models, use a cloud GPU or a Linux workstation with a recent NVIDIA card.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What‚Äôs the simplest way to load a base model in Unsloth?\",\n",
    "            \"output\": \"Use `FastLanguageModel.from_pretrained(model_name, load_in_4bit=True, max_seq_length=...)` to load a model and quantize it for QLoRA quickly.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"When should I choose LoRA vs QLoRA in Unsloth?\",\n",
    "            \"output\": \"Use LoRA if you have enough VRAM and want simplicity. Use QLoRA when VRAM is limited: base weights in 4-bit + small trainable LoRA adapters give strong results with minimal memory.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What do LoRA hyperparameters r, alpha and dropout do?\",\n",
    "            \"output\": \"r is the rank of the low-rank adapters (capacity), alpha scales the adapter update (similar to a learning-rate multiplier), and dropout randomly zeroes adapter activations to regularize training.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I pick target modules for LoRA with Unsloth?\",\n",
    "            \"output\": \"Use Unsloth‚Äôs convenience options like `target_modules='all-linear'` to cover common linear layers. Advanced users can pass a list of exact module names to control which layers get adapters.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I format my dataset for instruction tuning with Unsloth?\",\n",
    "            \"output\": \"Create plain text fields per sample with a stable prompt template, e.g. `### Instruction: ...\\\\n### Response: ...`. Keep formatting consistent across all samples.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What sequence length should I set in Unsloth?\",\n",
    "            \"output\": \"Choose the smallest `max_seq_length` that fits your task. Longer contexts increase VRAM and training time. Start with 512 or 1024 and scale up only if needed.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I enable efficient packing of short samples?\",\n",
    "            \"output\": \"If you use TRL‚Äôs `SFTTrainer`, set `packing=True`. This packs multiple short texts into a single sequence to reduce padding and speed up training.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Which optimizer is recommended for QLoRA in Unsloth?\",\n",
    "            \"output\": \"Paged optimizers (e.g., paged AdamW 8-bit) are commonly used because they reduce memory footprint. They work well with 4-bit quantization.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What learning rate should I start with for adapter tuning?\",\n",
    "            \"output\": \"Typical starting points are 1e-4 to 2e-4 for adapters. Use warmup (e.g., 50‚Äì200 steps) and monitor loss; lower LR if you see instability or overfitting.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do gradient accumulation and batch size affect memory?\",\n",
    "            \"output\": \"Larger batch sizes improve stability but need more VRAM. With limited VRAM, set a small per-device batch (e.g., 1‚Äì4) and increase `gradient_accumulation_steps` to reach an effective batch size.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Should I train in bf16 or fp16 with Unsloth?\",\n",
    "            \"output\": \"If your GPU supports bf16 well, prefer bf16 for stability. Otherwise fp16 is fine. On older cards without good bf16, stick to fp16.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I monitor training progress with Unsloth?\",\n",
    "            \"output\": \"Log loss, learning rate, and throughput. You can integrate Weights & Biases or use TRL/Transformers logging. Validate periodically on a held-out set of prompts.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What is the recommended prompt template style?\",\n",
    "            \"output\": \"Keep it simple and consistent, e.g., `### Instruction:\\\\n{user}\\\\n\\\\n### Response:\\\\n{assistant}`. Don‚Äôt mix multiple templates in one run unless you know what you‚Äôre doing.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I avoid overfitting during Unsloth fine-tuning?\",\n",
    "            \"output\": \"Use a validation split, early stopping or limited epochs, apply dropout in LoRA, and avoid training too long on tiny datasets. Monitor for verbatim memorization.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I run inference after training with adapters?\",\n",
    "            \"output\": \"Load the same base model with Unsloth and call `model.load_adapter('path/to/adapters')`. Tokenize your prompt and generate with your preferred decoding settings.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Can I merge LoRA adapters back into the base model?\",\n",
    "            \"output\": \"Yes. Merging produces a single model artifact (fp16/fp32) that‚Äôs simpler to deploy but loses the flexibility of swapping adapters.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What models are good starters for Unsloth fine-tuning?\",\n",
    "            \"output\": \"TinyLlama or Phi-mini for very small GPUs; Mistral-7B or Llama-8B for stronger baselines with QLoRA; choose an Instruct variant if you‚Äôre doing instruction tuning.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What dataset size is reasonable for a quick Unsloth demo?\",\n",
    "            \"output\": \"A few hundred to a few thousand examples is enough to see behavior changes. Start small to validate the pipeline, then scale up as needed.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I set `eos_token_id`, padding, and truncation correctly?\",\n",
    "            \"output\": \"Ensure the tokenizer has a defined EOS; set `padding_side='right'` for causal models; truncate to `max_seq_length`. Consistent EOS handling prevents run-on generations.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What generation settings should I test after fine-tuning?\",\n",
    "            \"output\": \"Try `max_new_tokens` (e.g., 64‚Äì256), `temperature` (0.2‚Äì0.8), `top_p` (0.9), and `do_sample=True`. For deterministic outputs, set sampling off and use greedy/beam search.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How does Unsloth help with VRAM limits?\",\n",
    "            \"output\": \"QLoRA with 4-bit base weights, 8-bit optimizers, packing, and kernel optimizations reduce memory usage, allowing larger models on modest GPUs.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How can I resume training if it stops midway?\",\n",
    "            \"output\": \"Point `output_dir` to the previous run, reload the trainer with the same config, and Unsloth/Transformers will restore weights, optimizer, and scheduler if checkpoints exist.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What evaluation approach should I use for small instruction datasets?\",\n",
    "            \"output\": \"Hold out 5‚Äì20% as a dev set; use simple exact-match or regex for structured tasks; optionally use an LLM-as-judge for open-ended quality checks.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I prepare conversational data for Unsloth?\",\n",
    "            \"output\": \"Either flatten to your text template or use a chat template to format messages consistently (system, user, assistant). Keep roles and separators stable.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What‚Äôs a good epoch count for adapter fine-tunes?\",\n",
    "            \"output\": \"Often 1‚Äì3 epochs are enough. With tiny datasets, prefer more steps via repeat+shuffle rather than many epochs to avoid rapid overfitting.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I handle long-context training with Unsloth?\",\n",
    "            \"output\": \"Increase `max_seq_length` cautiously and use packing. Expect higher memory and slower steps. Consider gradient checkpointing if supported to trade compute for memory.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What should I check if I hit CUDA OOM errors?\",\n",
    "            \"output\": \"Lower `max_seq_length`, reduce batch size, increase gradient accumulation, switch to QLoRA, or disable extra features. Verify no background processes are using VRAM.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Is it okay to mix tasks in one Unsloth fine-tune?\",\n",
    "            \"output\": \"Yes if they share style/instructions. Keep templates consistent and balance the dataset so one task doesn‚Äôt dominate unless intended.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I log train/val samples to W&B with Unsloth?\",\n",
    "            \"output\": \"Initialize W&B in your script, log losses and sample generations at evaluation steps, and attach key hyperparameters (LR, r, alpha, dropout, max_seq_length) as config.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What scheduler should I use for adapter training?\",\n",
    "            \"output\": \"Cosine or linear with warmup are common defaults. The choice matters less than using a reasonable LR and warmup; tune based on validation loss.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I choose the right rank r for LoRA?\",\n",
    "            \"output\": \"Start with r=8‚Äì16. Increase r if the model underfits or the task is complex; decrease if you observe overfitting or want smaller adapters.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What‚Äôs the best way to export the fine-tuned result?\",\n",
    "            \"output\": \"Keep adapters for modularity and small artifacts, or merge them into the base weights to get a single model for simpler deployment.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Can Unsloth be used with TRL‚Äôs SFTTrainer?\",\n",
    "            \"output\": \"Yes. Load the model with Unsloth, wrap LoRA, then pass the model and tokenizer into TRL‚Äôs `SFTTrainer` with your dataset and training args.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I set up a tiny demo run on a free GPU notebook?\",\n",
    "            \"output\": \"Use a small model (e.g., TinyLlama), set `max_seq_length=512`, QLoRA on, small batch size, gradient accumulation, and train for a few hundred steps to verify the pipeline.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What‚Äôs the role of a clean prompt template in fine-tuning?\",\n",
    "            \"output\": \"It standardizes context so the model learns consistent input-output mapping. Messy templates make training noisy and degrade quality.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I check that the model learned my style after fine-tuning?\",\n",
    "            \"output\": \"Create a small evaluation script that feeds 10‚Äì20 held-out prompts and inspects responses for instruction adherence, tone, and correctness.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What are common pitfalls when fine-tuning with Unsloth?\",\n",
    "            \"output\": \"Inconsistent templates, too-high LR, no validation set, too-long sequences on small GPUs, forgetting EOS handling, and mixing incompatible tokenizers/models.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"How do I keep runs reproducible?\",\n",
    "            \"output\": \"Fix random seeds, pin package versions, log all hyperparameters, and checkpoint regularly so you can resume with the same state.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format for instruction following with special tokens\n",
    "    formatted_data = []\n",
    "    for item in training_data:\n",
    "        text = f\"### Instruction: {item['instruction']}\\n### Response: {item['output']}<|endoftext|>\"\n",
    "        formatted_data.append({\"text\": text})\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_training_dataset()\n",
    "\n",
    "print(f\"üìö Created dataset with {len(dataset)} examples\")\n",
    "print(\"\\nüìù Sample training example:\")\n",
    "print(dataset[0][\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "Load the base model and tokenizer, then configure LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(model_name: str, lora_config: Dict):\n",
    "    \"\"\"Set up model, tokenizer, and LoRA configuration.\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "    \n",
    "    # Set pad token if not exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"üîß Set pad_token to eos_token\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=lora_config[\"lora_r\"],\n",
    "        lora_alpha=lora_config[\"lora_alpha\"],\n",
    "        target_modules=lora_config[\"target_modules\"],\n",
    "        lora_dropout=lora_config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = 100 * trainable_params / total_params\n",
    "    \n",
    "    print(f\"üéØ Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"üìä Total parameters: {total_params:,}\")\n",
    "    print(f\"üí° Trainable percentage: {trainable_percentage:.2f}%\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Setup model and tokenizer\n",
    "model, tokenizer = setup_model_and_tokenizer(config[\"model_name\"], config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset for Training\n",
    "\n",
    "Tokenize the dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length: int):\n",
    "    \"\"\"Tokenize the dataset for training.\"\"\"\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Set labels for causal language modeling\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"üîÑ Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset, tokenizer, config[\"max_length\"])\n",
    "\n",
    "print(f\"‚úÖ Dataset tokenized: {len(tokenized_dataset)} examples\")\n",
    "print(f\"üìè Max length: {config['max_length']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "Configure the training arguments and data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    num_train_epochs=config[\"num_epochs\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    warmup_steps=config[\"warmup_steps\"],\n",
    "    logging_steps=10,\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # Disable all external reporting (W&B, TensorBoard, etc.)\n",
    "    load_best_model_at_end=False,\n",
    "    dataloader_pin_memory=False,  # Better compatibility\n",
    "    dataloader_num_workers=0,     # Better compatibility\n",
    "    fp16=False,  # Disable for stability\n",
    "    disable_tqdm=False,  # Enable tqdm progress bars\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Training configuration set up:\")\n",
    "print(f\"   üìä Epochs: {config['num_epochs']}\")\n",
    "print(f\"   üéØ Batch size: {config['batch_size']}\")\n",
    "print(f\"   üìà Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   üíæ Output dir: {config['output_dir']}\")\n",
    "print(f\"   üìà Progress bars: Enabled with tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training with MLflow Tracking\n",
    "\n",
    "Now let's train the model and track everything with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for training\n",
    "with mlflow.start_run(run_name=\"lora_training\") as run:\n",
    "    \n",
    "    # Log all configuration parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "        \"max_length\": config[\"max_length\"],\n",
    "        \"batch_size\": config[\"batch_size\"],\n",
    "        \"learning_rate\": config[\"learning_rate\"],\n",
    "        \"num_epochs\": config[\"num_epochs\"],\n",
    "        \"lora_r\": config[\"lora_r\"],\n",
    "        \"lora_alpha\": config[\"lora_alpha\"],\n",
    "        \"lora_dropout\": config[\"lora_dropout\"],\n",
    "        \"target_modules\": str(config[\"target_modules\"]),\n",
    "        \"device\": str(device),\n",
    "        \"dataset_size\": len(tokenized_dataset)\n",
    "    })\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Starting LoRA fine-tuning...\")\n",
    "    print(f\"üìä Dataset size: {len(tokenized_dataset)} examples\")\n",
    "    print(f\"‚è±Ô∏è Estimated time: ~{config['num_epochs'] * 2} minutes\")\n",
    "    \n",
    "    # Record training start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    print(\"üíæ Saving model and tokenizer...\")\n",
    "    trainer.save_model(config[\"output_dir\"])\n",
    "    tokenizer.save_pretrained(config[\"output_dir\"])\n",
    "    \n",
    "    # Extract final metrics\n",
    "    if trainer.state.log_history:\n",
    "        final_loss = trainer.state.log_history[-1].get(\"train_loss\", 0)\n",
    "    else:\n",
    "        final_loss = 0\n",
    "    \n",
    "    # Log training metrics to MLflow\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": final_loss,\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"training_time_minutes\": training_time / 60,\n",
    "        \"total_training_steps\": trainer.state.global_step,\n",
    "        \"examples_per_second\": len(tokenized_dataset) * config[\"num_epochs\"] / training_time,\n",
    "    })\n",
    "    \n",
    "    # Log model artifacts\n",
    "    mlflow.log_artifacts(config[\"output_dir\"], \"model_checkpoints\")\n",
    "    \n",
    "    training_run_id = run.info.run_id\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"‚è±Ô∏è Training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"üìâ Final loss: {final_loss:.4f}\")\n",
    "    print(f\"üÜî Run ID: {training_run_id}\")\n",
    "    print(f\"üíæ Model saved to: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Let's test our fine-tuned model with some sample questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path: str, tokenizer, test_questions: List[str]):\n",
    "    \"\"\"Evaluate the fine-tuned model.\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Loading fine-tuned model for evaluation...\")\n",
    "    \n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"üß™ Evaluating {len(test_questions)} questions...\")\n",
    "    \n",
    "    # Use tqdm for progress tracking during evaluation\n",
    "    for i, question in enumerate(tqdm(test_questions, desc=\"Evaluating questions\")):\n",
    "        # Format prompt\n",
    "        prompt = f\"### Instruction: {question}\\n### Response:\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=200\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"response\": response,\n",
    "            \"response_length\": len(response.split())\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain deep learning in simple terms.\",\n",
    "    \"What are the benefits of using MLflow?\",\n",
    "    \"How does LoRA fine-tuning work?\",\n",
    "    \"What is the difference between training and inference?\"\n",
    "]\n",
    "\n",
    "# Evaluate model\n",
    "evaluation_results = evaluate_model(config[\"output_dir\"], tokenizer, test_questions)\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, result in enumerate(evaluation_results, 1):\n",
    "    print(f\"\\nüî∏ Question {i}: {result['question']}\")\n",
    "    print(f\"üí¨ Response: {result['response']}\")\n",
    "    print(f\"üìè Length: {result['response_length']} words\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Log Evaluation Results to MLflow\n",
    "\n",
    "Let's log our evaluation results and metrics to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log evaluation results to MLflow\n",
    "with mlflow.start_run(run_name=\"model_evaluation\") as eval_run:\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    avg_response_length = sum(r[\"response_length\"] for r in evaluation_results) / len(evaluation_results)\n",
    "    non_empty_responses = sum(1 for r in evaluation_results if r[\"response\"].strip())\n",
    "    response_rate = non_empty_responses / len(evaluation_results)\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"avg_response_length\": avg_response_length,\n",
    "        \"response_rate\": response_rate,\n",
    "        \"total_test_questions\": len(test_questions),\n",
    "        \"successful_responses\": non_empty_responses\n",
    "    })\n",
    "    \n",
    "    # Log evaluation parameters\n",
    "    mlflow.log_params({\n",
    "        \"training_run_id\": training_run_id,\n",
    "        \"model_path\": config[\"output_dir\"],\n",
    "        \"evaluation_temperature\": 0.7,\n",
    "        \"max_new_tokens\": 100\n",
    "    })\n",
    "    \n",
    "    # Save evaluation results as CSV\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    results_file = \"evaluation_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    mlflow.log_artifact(results_file, \"evaluation\")\n",
    "    \n",
    "    # Log sample responses as parameters\n",
    "    for i, result in enumerate(evaluation_results[:3]):\n",
    "        mlflow.log_param(f\"sample_question_{i+1}\", result[\"question\"][:100])\n",
    "        mlflow.log_param(f\"sample_response_{i+1}\", result[\"response\"][:200])\n",
    "    \n",
    "    print(\"üìä Evaluation metrics logged to MLflow:\")\n",
    "    print(f\"   üìè Average response length: {avg_response_length:.1f} words\")\n",
    "    print(f\"   ‚úÖ Response rate: {response_rate:.1%}\")\n",
    "    print(f\"   üìÅ Results saved to: {results_file}\")\n",
    "    print(f\"   üÜî Evaluation run ID: {eval_run.info.run_id}\")\n",
    "\n",
    "# Clean up the temporary file\n",
    "if os.path.exists(results_file):\n",
    "    os.remove(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Registration (Optional)\n",
    "\n",
    "Register the trained model in MLflow Model Registry for version control and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model_in_registry(model_path: str, model_name: str = \"lora_finetuned_model\"):\n",
    "    \"\"\"Register the LoRA model in MLflow Model Registry.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with mlflow.start_run(run_name=\"model_registration\"):\n",
    "            \n",
    "            # Log the model directory as an artifact first\n",
    "            mlflow.log_artifacts(model_path, \"model\")\n",
    "            \n",
    "            # Log model metadata\n",
    "            mlflow.log_params({\n",
    "                \"base_model\": config[\"model_name\"],\n",
    "                \"lora_r\": config[\"lora_r\"],\n",
    "                \"lora_alpha\": config[\"lora_alpha\"],\n",
    "                \"training_run_id\": training_run_id,\n",
    "                \"model_type\": \"LoRA fine-tuned\"\n",
    "            })\n",
    "            \n",
    "            print(f\"üè∑Ô∏è Model artifacts logged for registration\")\n",
    "            print(f\"üì¶ Model name: {model_name}\")\n",
    "            print(f\"üîó Base model: {config['model_name']}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Model registration encountered an issue: {e}\")\n",
    "        print(\"üí° This is normal - model registry is optional for this demo\")\n",
    "        return False\n",
    "\n",
    "# Register the model\n",
    "registration_success = register_model_in_registry(config[\"output_dir\"])\n",
    "\n",
    "if registration_success:\n",
    "    print(\"‚úÖ Model registration completed!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Model saved locally and ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Let's summarize what we accomplished and suggest next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ LoRA Fine-tuning Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã What we accomplished:\")\n",
    "print(f\"   ‚úÖ Fine-tuned {config['model_name']} with LoRA\")\n",
    "print(f\"   ‚úÖ Trained on {len(dataset)} instruction-following examples\")\n",
    "print(f\"   ‚úÖ Used only {config['lora_r']} rank (very efficient!)\")\n",
    "print(f\"   ‚úÖ Tracked everything with MLflow (no external dependencies)\")\n",
    "print(f\"   ‚úÖ Evaluated model on {len(test_questions)} test questions\")\n",
    "print(f\"   ‚úÖ Saved model to: {config['output_dir']}\")\n",
    "\n",
    "print(\"\\nüìä Key Results:\")\n",
    "print(f\"   üìâ Final training loss: {final_loss:.4f}\")\n",
    "print(f\"   ‚è±Ô∏è Training time: {training_time/60:.1f} minutes\")\n",
    "print(f\"   üìè Avg response length: {avg_response_length:.1f} words\")\n",
    "print(f\"   ‚úÖ Response rate: {response_rate:.1%}\")\n",
    "\n",
    "print(\"\\nüîó MLflow Integration (Local Only):\")\n",
    "print(f\"   üìä Experiment: {config['experiment_name']}\")\n",
    "print(f\"   üèÉ Training run: {training_run_id[:8]}...\")\n",
    "print(f\"   üß™ Evaluation run: {eval_run.info.run_id[:8]}...\")\n",
    "print(f\"   üìà Progress tracking: tqdm enabled\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. üåê View detailed results: mlflow ui\")\n",
    "print(\"   2. üéØ Try different LoRA configurations\")\n",
    "print(\"   3. üìö Expand training dataset for better performance\")\n",
    "print(\"   4. üîÑ Compare with other fine-tuning methods\")\n",
    "print(\"   5. üöÄ Deploy model for production use\")\n",
    "\n",
    "print(\"\\nüí° Useful Commands:\")\n",
    "print(\"   ‚Ä¢ Start MLflow UI: mlflow ui\")\n",
    "print(\"   ‚Ä¢ Access at: http://localhost:5000\")\n",
    "print(\"   ‚Ä¢ Model location:\", Path(config[\"output_dir\"]).absolute())\n",
    "\n",
    "print(\"\\nüéì What you learned:\")\n",
    "print(\"   ‚Ä¢ LoRA fine-tuning is very parameter-efficient\")\n",
    "print(\"   ‚Ä¢ MLflow provides comprehensive local experiment tracking\")\n",
    "print(\"   ‚Ä¢ tqdm gives clear progress visualization during training\")\n",
    "print(\"   ‚Ä¢ Small models can be effective for specific tasks\")\n",
    "print(\"   ‚Ä¢ Evaluation is crucial for model assessment\")\n",
    "print(\"   ‚Ä¢ No external tracking services (W&B) needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Happy fine-tuning with MLflow! ü§ñ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-learn-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
